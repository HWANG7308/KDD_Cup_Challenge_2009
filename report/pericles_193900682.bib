
















@article{doi:10.1002/wics.55,
author = {Bühlmann, Peter and Yu, Bin},
title = {Boosting},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
volume = {2},
number = {1},
pages = {69-74},
keywords = {boosting, gradient descent, AdaBoost, L2Boost, base learner},
doi = {10.1002/wics.55},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.55},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.55},
abstract = {Abstract In this contribution, we review boosting, one of the most effective machine learning methods for classification and regression. Most of the article takes the gradient descent point of view, even though we do include the margin point of view as well. In particular, AdaBoost in classification and various versions of L2boosting in regression are covered. Advice on how to choose base (weak) learners and loss functions and pointers to software are also given for practitioners. Copyright © 2009 John Wiley \& Sons, Inc. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Classification and Regression Trees (CART)},,
year = {2010}
}

